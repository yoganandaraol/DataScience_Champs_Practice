{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/insaid2018/Term-1/blob/master/Images/INSAID_Full%20Logo.png?raw=true\" width=\"240\" height=\"360\" />\n",
    "\n",
    "# ASSIGNMENT\n",
    "# DECISION TREES AND RANDOM FOREST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring publicly available data from __LendingClub.com__. Lending Club connects people who need money __(borrowers)__ with people who have money __(investors)__.I am trying to create a model that will help predict people who have a profile of having a __high probability of paying back__.\n",
    "\n",
    "Lending club had a very interesting year in __2016__. This data is from _before they even went public_.\n",
    "\n",
    "I'm using lending data from __2007-2010__ and trying to classify and predict whether or not the borrower paid back their loan in full. The data is downloaded from here\n",
    "\n",
    "Here are what the columns represent:\n",
    "- __credit.policy:__ 1 if the customer meets the credit underwriting criteria of LendingClub.com, and 0 otherwise.\n",
    "- __purpose:__ The purpose of the loan (takes values \"credit_card\", \"debt_consolidation\", \"educational\", \"major_purchase\", \"small_business\", and \"all_other\").\n",
    "- __int.rate:__ The interest rate of the loan, as a proportion (a rate of 11% would be stored as 0.11). Borrowers judged by LendingClub.com to be more risky are assigned higher interest rates.\n",
    "- __installment:__ The monthly installments owed by the borrower if the loan is funded.\n",
    "- __log.annual.inc:__ The natural log of the self-reported annual income of the borrower.\n",
    "- __dti:__ The debt-to-income ratio of the borrower (amount of debt divided by annual income).\n",
    "- __fico:__ The FICO credit score of the borrower.\n",
    "- __days.with.cr.line:__ The number of days the borrower has had a credit line.\n",
    "- __revol.bal:__ The borrower's revolving balance (amount unpaid at the end of the credit card billing cycle).\n",
    "- __revol.util:__ The borrower's revolving line utilization rate (the amount of the credit line used relative to total credit available).\n",
    "- __inq.last.6mths:__ The borrower's number of inquiries by creditors in the last 6 months.\n",
    "- __delinq.2yrs:__ The number of times the borrower had been 30+ days past due on a payment in the past 2 years.\n",
    "- __pub.rec:__ The borrower's number of derogatory public records (bankruptcy filings, tax liens, or judgments)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "loans = pd.read_csv('https://raw.githubusercontent.com/insaid2018/Term-2/master/Data/loan_data.csv')\n",
    "loans.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the columns present in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the shape of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the descriptive statistics of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the info of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 1. Write a code to check if there is any missing values present in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree():\n",
    "    return #your code to check the missing values\n",
    "tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 2. Plot a histogram of two FICO distribution on top of each other, one for each credit.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def tree():\n",
    "    # set plot figure size\n",
    "    # code to create histogram for Credit.Policy=1 having alpha = 0.5, bins=30 and provide proper label\n",
    "                                              \n",
    "    # code to create histogram for Credit.Policy=0 having alpha = 0.5, bins=30 and provide proper label\n",
    "                                              \n",
    "    # provide legend\n",
    "    # provide xlabel\n",
    "tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 3. Plot a histogram of two FICO distribution on top of each other, one for each not.fully.paid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def tree():\n",
    "    # set plot figure size\n",
    "    # code to create histogram for not.fully.paid=1 having alpha = 0.5, bins=30 and provide proper label\n",
    "                                              \n",
    "    # code to create histogram for not.fully.paid=0 having alpha = 0.5, bins=30 and provide proper label\n",
    "                                              \n",
    "    # provide legend\n",
    "    # provide xlabel\n",
    "tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 4. Using seaborn countplot show the counts of loans by purpose, with the color hue defined by not.fully.paid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "def tree():\n",
    "    # set plot figure size\n",
    "    # your code to create the countplot\n",
    "tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q. 5 Using a seaborn countplot understand the trend between FICO score and interest rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "def tree():\n",
    "    # set plot figure size\n",
    "    # your code to create the countplot\n",
    "tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with categorical variables.\n",
    "``` ['purpose'] ```\n",
    "- The purpose column is categorical. so transform them using dummy variables using pd.get_dummies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_feats = ['purpose']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 6. Using pd.get_dummies create a fixed larger dataframe that has new feature columns with dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = pd.DataFrame()\n",
    "def tree():\n",
    "    # create a new data frame final data with dummies of the categorical variables\n",
    "    return final_data\n",
    "final_data = tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 7. Extract the independent column to prepare X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame()\n",
    "def tree():\n",
    "    # create new dataframe X having all the independebt variables\n",
    "    return #newly created dataframe\n",
    "X = tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 8. Extract dependent column into a dataframe y for model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.DataFrame()\n",
    "def tree():\n",
    "     # create new dataframe y having the dependebt variable\n",
    "    return #newly created dataframe\n",
    "y = tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 9. Split X and y into train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def tree():\n",
    "    return #using train_test_split, split the data into train and test in the ration 70:30 respectively.\n",
    "X_train, X_test, y_train, y_test = tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the shape of X and y of train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree():\n",
    "    print(X_train.shape)\n",
    "    print(y_train.shape)\n",
    "tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the shape of X and y of test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree():\n",
    "    print(X_test.shape)\n",
    "    print(y_test.shape)\n",
    "tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 10. Instantiate Decision Tree Classifier using scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "def tree():\n",
    "    # initate the DecisionTreeClasifier as dtree.\n",
    "    return dtree\n",
    "dtree = tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 11. Fit the model on X_train and y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree():\n",
    "    return # the fitted model on X_train and y_train\n",
    "tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 12. Using the model for prediciton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame()\n",
    "def tree():\n",
    "    # use the model for prediction and assign it to predictions\n",
    "    return predictions\n",
    "predictions = tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 13. Model evaluation using Confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "def tree():\n",
    "    #COmpute and print the confusion matrix\n",
    "tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 14. Model evaluation using recall score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "def log():\n",
    "    # compute the recall score as recall1\n",
    "    # your code to print the recall1 scoreprint('Recall score for test data is:', recall1)\n",
    "    return recall1\n",
    "recall1 = log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 15. Model evaluation using classification report.\n",
    "- Please refer the link for Classification report documentation - http://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "def tree():\n",
    "    # COmpute and print the classification report\n",
    "tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 16. Instantiate Decision Tree Classifer using sikit learn having (criterion='entropy', max_leaf_nodes=10, max_depth=3, min_samples_split=5, min_samples_leaf=4).\n",
    "- Please refer the link for DecisionTreeClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree():\n",
    "    # instantiate Decision tree classifier as dtree1 with the above mentioned parameters\n",
    "    return dtree1\n",
    "dtree1 = tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16.1 Fit the model X_train and y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree():\n",
    "    return # the fitted model on X_train and y_train\n",
    "tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16.2 Use the model for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_new = pd.DataFrame()\n",
    "def tree():\n",
    "    # use the model for prediction and assign it to predeictions_new\n",
    "    return predictions_new\n",
    "predictions_new = tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16.3 Model evaluation using Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree():\n",
    "    # Compute and print the confusion matrix\n",
    "tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16.4 Model evaluation using recall score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "def log():\n",
    "    # Compute the recall score as recall2\n",
    "    # print the recall score for the recall2\n",
    "    return recall2\n",
    "recall2 = log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16.5 Model evaluation using classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree():\n",
    "    # Compute and print the classification report\n",
    "tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 17. Write a code to find the difference between the recall score of 2 Decision Tree models created above ( Give the absolute value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log():\n",
    "    # Compute the difference between recall1 and recall2\n",
    "    return # the difference value\n",
    "log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 18. Instantiate Random Forest Classifier using scikit learn having n_estimators = 600."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "def rf():\n",
    "    # instantiate the random forest classifier and assign it to rfc\n",
    "    return rfc\n",
    "rfc = rf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 19. Fit the model on X_train and y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf():\n",
    "    return # the fitted model on X_train and y_train\n",
    "rf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 20. Using the model for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions1 = pd.DataFrame()\n",
    "def rf():\n",
    "    # use the model for predictions and assign it to predictions1\n",
    "    return predictions1\n",
    "predictions1 = rf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 21. Model evaluation using Confusion matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf():\n",
    "    # COmpute and print the confusion matrix\n",
    "rf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 22. Model evaluation using recall score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "def log():\n",
    "    # Compute the recall score and assign it to recall_rfc\n",
    "    # print the recall_rfc\n",
    "    return recall_rfc\n",
    "recall_rfc = log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 23. Model evaluation using Classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf():\n",
    "    # Compute and print classification report\n",
    "rf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 24. Instantitate Random Forest Model using scikit learn having (criterion='entropy',n_estimators = 100, random_state = 0, max_depth = 2, min_samples_split=4, min_samples_leaf=3, max_leaf_nodes=5).\n",
    "- Pleae refer the link for RandomForestClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf():\n",
    "    # instantiate the random forest classifier as rfc_new using all the parameters mentioned above\n",
    "    return rfc_new\n",
    "rfc_new = rf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 24.1 Fit the model on X_train and y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf():\n",
    "    return # the fiited model on X_train and y_train\n",
    "rf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 24.2 Using the model for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_predictions = pd.DataFrame()\n",
    "def rf():\n",
    "    # using the model for predictions and assign it to new_predictions\n",
    "    return new_predictions\n",
    "new_predictions = rf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 24.3 Model evaluation using Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf():\n",
    "    # Compute and print the confusion matrix\n",
    "rf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 24.4 Model evaluation using recall score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "def log():\n",
    "    # Compute the recall_score as recall_rfc1\n",
    "    # printt the recall score\n",
    "    return recall_rfc1\n",
    "recall_rfc1 = log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 24.5 Model evaluation using Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf():\n",
    "    # Compute and print the classification report\n",
    "rf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 25. Write a code to understand the difference between the recall score of the above 2 Random Forest models (Give the absolute value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log():\n",
    "    # Calculate the difference between recall_rfc and recall_rfc1\n",
    "    return # the difference value\n",
    "log()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
